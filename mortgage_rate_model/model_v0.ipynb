{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Rate Model\n",
    "\n",
    "This is the first model being built using the model building framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Function Imports, and Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key='1e208e2d66ac6382c25f85524a5820cc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_fred_data_as_dataframe(series_code = str):\n",
    "    '''Downloads and cleans FRED API call into dataframe.'''\n",
    "\n",
    "    # Download the series using fredapi\n",
    "    fred_series = fred.get_series(series_code)\n",
    "\n",
    "    # Convert the Series into a DataFrame and move the index into a column\n",
    "    df_raw = fred_series.reset_index()\n",
    "\n",
    "    # Convert the series code to lowercase for the column name\n",
    "    lowercase_series_code = series_code.lower()\n",
    "\n",
    "    # Explicitly rename columns by position to avoid guessing the column name\n",
    "    df_raw.columns = [\"date\", lowercase_series_code]\n",
    "\n",
    "    # Convert date column to datetime\n",
    "    df_raw['date'] = pd.to_datetime(df_raw['date'])\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intake mortgage rate time series\n",
    "df_mortgage_rate_weekly = download_fred_data_as_dataframe('MORTGAGE30US')\n",
    "\n",
    "print(df_mortgage_rate_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Year Treasury Yield\n",
    "df_ten_year_yield_daily = download_fred_data_as_dataframe('DGS10')\n",
    "\n",
    "print(df_ten_year_yield_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Year Treasury Yield\n",
    "df_two_year_yield_daily = download_fred_data_as_dataframe('DGS2')\n",
    "\n",
    "print(df_two_year_yield_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10Y–2Y spread\n",
    "df_ten_two_yield_spread_daily = download_fred_data_as_dataframe('T10Y2Y')\n",
    "\n",
    "print(df_ten_two_yield_spread_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective Federal funds rate\n",
    "df_federal_funds_rate_daily = download_fred_data_as_dataframe('DFF')\n",
    "\n",
    "print(df_federal_funds_rate_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "Follow the formatting of FRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe with dates, then merge all data onto it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = pd.DataFrame({\n",
    "    \"date\": pd.date_range(start=\"1900-01-01\", end=pd.Timestamp.today(), freq=\"D\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge\n",
    "\n",
    "Merge all data into one big dataframe. We start using a dataframe that contains every date from 1900 to present. We then merge every dataframe containing data (target and explanatory) onto this clean date dataframe. The result is a clean dataframe where we can easily delete each row that does not contain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all dataframes that contain data that will be in our model.\n",
    "list_data_dfs = [\n",
    "    df_mortgage_rate_weekly, \n",
    "    df_ten_year_yield_daily, \n",
    "    df_two_year_yield_daily, \n",
    "    df_ten_two_yield_spread_daily, \n",
    "    df_federal_funds_rate_daily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dates_with_data(base_dates_dataframe, data_dataframe):\n",
    "    # Merge a single data dataframe onto the base dates dataframe.\n",
    "    # This function always returns a NEW dataframe and does not modify inputs.\n",
    "    # An outer merge is used so no dates are lost.\n",
    "    merged_dataframe = base_dates_dataframe.merge(\n",
    "        data_dataframe,\n",
    "        how=\"outer\",\n",
    "        on=\"date\"\n",
    "    )\n",
    "\n",
    "    return merged_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the master date dataframe\n",
    "df_all_data = df_dates.copy()\n",
    "\n",
    "# Merge each dataframe one at a time (explicitly, no loop)\n",
    "df_all_data = merge_dates_with_data(df_all_data, df_mortgage_rate_weekly)\n",
    "df_all_data = merge_dates_with_data(df_all_data, df_ten_year_yield_daily)\n",
    "df_all_data = merge_dates_with_data(df_all_data, df_two_year_yield_daily)\n",
    "df_all_data = merge_dates_with_data(df_all_data, df_ten_two_yield_spread_daily)\n",
    "df_all_data = merge_dates_with_data(df_all_data, df_federal_funds_rate_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the DataFrame\n",
    "\n",
    "To make this super simple on my self, we will assure datatypes for the whole dataframe, and standardize missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Target column\n",
    "target_column_name = 'mortgage30us'\n",
    "\n",
    "df_all_data = df_all_data.rename(columns={target_column_name : 'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the target is not present.\n",
    "df_all_data.dropna(subset=[\"target\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assure that numeric cols are numeric and have standardized NaN values\n",
    "numeric_cols = df_all_data.columns.difference(['date'])\n",
    "df_all_data[numeric_cols] = (df_all_data[numeric_cols].apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering and Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12/21/2025 note: VIF is high between most variables. Highest correlation with dgs10. For sake of getting started, I will do a basic model of dgs10 on mortgage rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: After doing a review of VIF and correlation:\n",
    "df_model_data = df_all_data.copy()\n",
    "\n",
    "# df_model_data = df_model_data[['date', 'target', 'dgs10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "\n",
    "This shows us how closesly correlated two variables are in our data. (> .50 or < -.5 represents higher correlation)\n",
    "\n",
    "If two variables are correlated, it could represent a possible explanatory relationship.\n",
    "\n",
    "- The most important variable to have high correlation is with the 'target'.\n",
    "\n",
    "- If two variables THAT ARE NOT THE 'target' are highly correlated, we could have multicollinearity (bad), so we need to test VIF (below).\n",
    "\n",
    "> If the correlation is positive, when one variable goes up, so does the other. Same vice versa, if the correlation is negative, when one variable goes up, the other goes down. Think about if this makes sense intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_model_data.drop(columns=[\"date\"]).corr()\n",
    "\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation over time\n",
    "\n",
    "The matrix above show us the correlation over all of history, but the relationship between variables can change over time. These graphs display how the correlation between the variables and the target changes over time, with a 95% confidence interval. A variable getting close to or crossing 0% could mean that the relationship is not strong, and should probably be removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- hardcode these ---\n",
    "df = df_model_data.copy()\n",
    "WINDOW = 30\n",
    "START_DATE = \"2009-01-01\"   # or None\n",
    "END_DATE   = None # \"2024-12-31\"   # or None\n",
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"date\").set_index(\"date\")\n",
    "num = df.select_dtypes(include=\"number\")\n",
    "\n",
    "feature_cols = [c for c in num.columns if c != \"target\"]\n",
    "\n",
    "# rolling correlation (index=date, columns=features)\n",
    "corr = num[feature_cols].rolling(WINDOW).corr(num[\"target\"])\n",
    "\n",
    "# rolling sample size (pairwise non-NaN count per window)\n",
    "n = pd.concat(\n",
    "    {\n",
    "        c: num[[c, \"target\"]].notna().all(axis=1).rolling(WINDOW).sum()\n",
    "        for c in feature_cols\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# hardcoded date filter\n",
    "if START_DATE is not None:\n",
    "    start_ts = pd.Timestamp(START_DATE)\n",
    "    corr = corr.loc[corr.index >= start_ts]\n",
    "    n = n.loc[corr.index]\n",
    "\n",
    "if END_DATE is not None:\n",
    "    end_ts = pd.Timestamp(END_DATE)\n",
    "    corr = corr.loc[corr.index <= end_ts]\n",
    "    n = n.loc[corr.index]\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "ZCRIT = 1.959963984540054  # ~95% CI\n",
    "\n",
    "def corr_ci(r, n_):\n",
    "    r = np.asarray(r, float)\n",
    "    n_ = np.asarray(n_, float)\n",
    "\n",
    "    lo = np.full_like(r, np.nan)\n",
    "    hi = np.full_like(r, np.nan)\n",
    "\n",
    "    ok = np.isfinite(r) & np.isfinite(n_) & (n_ >= 4) & (np.abs(r) < 1)\n",
    "    if ok.any():\n",
    "        z = np.arctanh(r[ok])\n",
    "        se = 1.0 / np.sqrt(n_[ok] - 3.0)\n",
    "        lo[ok] = np.tanh(z - ZCRIT * se)\n",
    "        hi[ok] = np.tanh(z + ZCRIT * se)\n",
    "\n",
    "    return lo, hi\n",
    "\n",
    "def plot_corr(feature):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    r = corr[feature]\n",
    "    nn = n[feature]\n",
    "    lo, hi = corr_ci(r.values, nn.values)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 3.2))\n",
    "    ax.plot(r.index, r.values, color=\"#1f77b4\", linewidth=2)\n",
    "    ax.fill_between(r.index, lo, hi, color=\"#1f77b4\", alpha=0.18, linewidth=0)\n",
    "\n",
    "    ax.axhline(0, color=\"black\", linewidth=1, alpha=0.6)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_title(f\"{feature} vs target — rolling {WINDOW} corr (+ ~95% CI)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Correlation\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "widgets.interact(\n",
    "    plot_corr,\n",
    "    feature=widgets.Dropdown(options=feature_cols)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (VIF)\n",
    "\n",
    "This tests for multicollinearity between columns. Multicollinearity is when two data sources are similar to, or the same, as one another. Multicollinearity is a problem for Linear and Polynomial Regression models. So if the model is only being solved using a basic regression, we should assure little to no multicollinearity.\n",
    "\n",
    "Care about VIF if:\n",
    "- You’re doing inference\n",
    "- You care about coefficient signs/magnitudes\n",
    "- You’re publishing / explaining results\n",
    "- You’re using linear or logistic regression\n",
    "\n",
    "Don’t care much if:\n",
    "- You only care about prediction\n",
    "- You’re using trees / boosting / RF\n",
    "- You use regularization (ridge handles this)\n",
    "\n",
    "> Values near 1 mean the variable is largely independent; values above ~5–10 indicate strong multicollinearity, and inf means exact redundancy that must be fixed. High VIF affects coefficient stability and interpretability, not predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 12/21 - Skipping because we no longer have lots of variables.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "df_independents = df_model_data.apply(pd.to_numeric, errors=\"coerce\")\n",
    "df_independents = df_independents.drop(columns=['date', 'target'])\n",
    "\n",
    "X = df_independents.dropna().to_numpy()\n",
    "\n",
    "vif = pd.Series(\n",
    "    [variance_inflation_factor(X, i) for i in range(X.shape[1])],\n",
    "    index=df_independents.columns,\n",
    "    name=\"VIF\",\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "vif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations out should be predicted.\n",
    "PREDICTION_HORIZON = 1\n",
    "GROWTH_OR_LEVEL = 'growth' #'growth' or 'level'\n",
    "\n",
    "df_model = df_model_data.copy()\n",
    "\n",
    "if GROWTH_OR_LEVEL == 'level':\n",
    "    # Shift the target and date by the prediction horizon\n",
    "    df_model['target'] = df_model['target'].shift(PREDICTION_HORIZON)\n",
    "    df_model['target_date'] = df_model['date'].shift(PREDICTION_HORIZON)\n",
    "    df_model['feature_date'] = df_model['date']\n",
    "    df_model.drop(columns=['date'])\n",
    "\n",
    "    # Drop missing data due to the shift\n",
    "    df_model.dropna(subset=['target'], inplace=True)\n",
    "\n",
    "else:  # growth\n",
    "    # Columns to convert to growth\n",
    "    growth_cols = [c for c in df_model.columns if c not in ['target', 'date']]\n",
    "\n",
    "    # Convert features to growth\n",
    "    df_model[growth_cols] = df_model[growth_cols].pct_change(fill_method=None)\n",
    "\n",
    "    # Convert target to growth separately\n",
    "    df_model['target'] = df_model['target'].pct_change(fill_method=None)\n",
    "\n",
    "    # Shift the target and date by the prediction horizon\n",
    "    df_model['target'] = df_model['target'].shift(PREDICTION_HORIZON)\n",
    "    df_model['target_date'] = df_model['date'].shift(PREDICTION_HORIZON)\n",
    "    df_model['feature_date'] = df_model['date']\n",
    "    df_model.drop(columns=['date'])\n",
    "\n",
    "    # Drop missing data due to the shift\n",
    "    df_model.dropna(subset=['target'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 12/29/2025 - Skipping for now, because we are trying to do xgboost\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# target_var = 'target'\n",
    "# comparison_var = 'dgs10'\n",
    "\n",
    "# df_plot = df_model[[comparison_var, target_var]].dropna()\n",
    "\n",
    "# plt.figure()\n",
    "# sns.regplot(\n",
    "#     data=df_plot,\n",
    "#     x=comparison_var,\n",
    "#     y=target_var,\n",
    "#     ci=95\n",
    "# )\n",
    "# plt.title(\"target vs dgs10 with OLS fit and 95% CI\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 12/29/2025 - Skipping for now, because we are trying to do xgboost\n",
    "\n",
    "# import statsmodels.formula.api as smf\n",
    "\n",
    "# model = smf.ols(formula='target ~ dgs10', data=df_model).fit()\n",
    "\n",
    "# # model summary\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xgboost\"       # \"xgboost\" or \"ridge\"\n",
    "WINDOW_TYPE = \"expanding\"    # \"expanding\" or \"rolling\"\n",
    "ROLLING_WINDOW_SIZE = 500    # only used if WINDOW_TYPE == \"rolling\"\n",
    "MIN_TRAIN_ROWS = 200\n",
    "\n",
    "df_model[\"feature_date\"] = pd.to_datetime(df_model[\"feature_date\"])\n",
    "df_model[\"target_date\"] = pd.to_datetime(df_model[\"target_date\"])\n",
    "df_model = df_model.sort_values(\"feature_date\").reset_index(drop=True)\n",
    "\n",
    "feature_columns = [c for c in df_model.columns if c not in [\"feature_date\", \"target_date\", \"target\"]]\n",
    "\n",
    "# Keep it strict: models require numeric features. If you have non-numeric, encode/drop before this.\n",
    "non_numeric_feature_columns = df_model[feature_columns].select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric_feature_columns:\n",
    "    raise ValueError(f\"Non-numeric feature columns found: {non_numeric_feature_columns}\")\n",
    "\n",
    "X_all_features = df_model[feature_columns]\n",
    "y_all_target = df_model[\"target\"]\n",
    "\n",
    "if MODEL_NAME.lower() in {\"xgb\", \"xgboost\"}:\n",
    "    model_template = XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=42,\n",
    "    )\n",
    "elif MODEL_NAME.lower() == \"ridge\":\n",
    "    model_template = Ridge(alpha=1.0)\n",
    "else:\n",
    "    raise ValueError(\"MODEL_NAME must be 'xgboost' or 'ridge'\")\n",
    "\n",
    "predictions = np.full(len(df_model), np.nan)\n",
    "\n",
    "for cutoff_feature_date in df_model[\"feature_date\"].drop_duplicates():\n",
    "    training_rows = df_model.index[df_model[\"feature_date\"] < cutoff_feature_date]\n",
    "    prediction_rows = df_model.index[df_model[\"feature_date\"] == cutoff_feature_date]\n",
    "\n",
    "    if len(training_rows) < MIN_TRAIN_ROWS or len(prediction_rows) == 0:\n",
    "        continue\n",
    "\n",
    "    if WINDOW_TYPE == \"rolling\":\n",
    "        training_rows = training_rows[-ROLLING_WINDOW_SIZE:]\n",
    "    elif WINDOW_TYPE != \"expanding\":\n",
    "        raise ValueError(\"WINDOW_TYPE must be 'expanding' or 'rolling'\")\n",
    "\n",
    "    model = model_template.__class__(**model_template.get_params())  # fresh copy each step\n",
    "    model.fit(X_all_features.loc[training_rows], y_all_target.loc[training_rows])\n",
    "    predictions[prediction_rows] = model.predict(X_all_features.loc[prediction_rows])\n",
    "\n",
    "backtest_results = df_model[[\"feature_date\", \"target_date\", \"target\"]].copy()\n",
    "backtest_results[\"prediction\"] = predictions\n",
    "backtest_results = backtest_results.dropna(subset=[\"prediction\"]).reset_index(drop=True)\n",
    "backtest_results[\"error\"] = backtest_results[\"prediction\"] - backtest_results[\"target\"]\n",
    "\n",
    "rmse = mean_squared_error(backtest_results[\"target\"], backtest_results[\"prediction\"], squared=False)\n",
    "mae = mean_absolute_error(backtest_results[\"target\"], backtest_results[\"prediction\"])\n",
    "\n",
    "print(f\"MODEL_NAME={MODEL_NAME}  WINDOW_TYPE={WINDOW_TYPE}\")\n",
    "print(f\"RMSE={rmse:.6f}  MAE={mae:.6f}\")\n",
    "\n",
    "# backtest_results has one row per prediction, time-safe\n",
    "# backtest_results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
